"""
YOLO11 ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í•™ìŠµ ë° íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ
ë” ì •í™•í•œ ê°ì²´ ê²€ì¶œì„ ìœ„í•œ ë§ì¶¤í˜• í•™ìŠµ ë„êµ¬
"""

import os
import json
import shutil
import numpy as np
import cv2
from pathlib import Path
from ultralytics import YOLO
import torch
import yaml
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class CustomDatasetPreparer:
    """
    ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    YOLO í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜ ë° êµ¬ì„±
    """
    
    def __init__(self, dataset_name="custom_dataset"):
        """
        ì´ˆê¸°í™”
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        """
        self.dataset_name = dataset_name
        self.base_path = Path(f"datasets/{dataset_name}")
        self.annotations = []
        self.class_names = []
        self.image_paths = []
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        self._create_directory_structure()
        
    def _create_directory_structure(self):
        """YOLO í•™ìŠµìš© ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        dirs = [
            self.base_path / "images" / "train",
            self.base_path / "images" / "val",
            self.base_path / "images" / "test",
            self.base_path / "labels" / "train",
            self.base_path / "labels" / "val",
            self.base_path / "labels" / "test",
        ]
        
        for dir_path in dirs:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ: {self.base_path}")
    
    def add_custom_classes(self, class_names: List[str]):
        """
        ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì¶”ê°€
        
        Args:
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        self.class_names = class_names
        print(f"âœ… {len(class_names)}ê°œì˜ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì¶”ê°€:")
        for i, name in enumerate(class_names):
            print(f"   {i}: {name}")
    
    def prepare_annotation_data(self, images_dir: str, annotations_file: str,
                               format_type="coco"):
        """
        ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ì¤€ë¹„
        
        Args:
            images_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            annotations_file: ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ê²½ë¡œ
            format_type: ì–´ë…¸í…Œì´ì…˜ í˜•ì‹ (coco, pascal_voc, yolo)
        """
        if format_type == "coco":
            self._convert_from_coco(annotations_file)
        elif format_type == "pascal_voc":
            self._convert_from_pascal_voc(annotations_file)
        else:
            print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format_type}")
            return
        
        # ì´ë¯¸ì§€ ë³µì‚¬
        self._copy_images(images_dir)
        
    def _convert_from_coco(self, coco_file):
        """COCO í˜•ì‹ì—ì„œ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        with open(coco_file, 'r') as f:
            coco_data = json.load(f)
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_map = {}
        for cat in coco_data['categories']:
            category_map[cat['id']] = cat['name']
        
        # ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”
        img_annotations = {}
        for ann in coco_data['annotations']:
            img_id = ann['image_id']
            if img_id not in img_annotations:
                img_annotations[img_id] = []
            img_annotations[img_id].append(ann)
        
        # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for img_info in coco_data['images']:
            img_id = img_info['id']
            img_width = img_info['width']
            img_height = img_info['height']
            
            if img_id in img_annotations:
                yolo_annotations = []
                for ann in img_annotations[img_id]:
                    # COCO bbox: [x, y, width, height]
                    bbox = ann['bbox']
                    x_center = (bbox[0] + bbox[2]/2) / img_width
                    y_center = (bbox[1] + bbox[3]/2) / img_height
                    width = bbox[2] / img_width
                    height = bbox[3] / img_height
                    
                    # YOLO format: class_id x_center y_center width height
                    class_id = ann['category_id'] - 1  # 0-indexed
                    yolo_annotations.append(f"{class_id} {x_center} {y_center} {width} {height}")
                
                self.annotations.append({
                    'image': img_info['file_name'],
                    'labels': yolo_annotations
                })
    
    def split_dataset(self, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):
        """
        ë°ì´í„°ì…‹ ë¶„í• 
        
        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            test_ratio: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        """
        total = len(self.annotations)
        indices = list(range(total))
        np.random.shuffle(indices)
        
        train_end = int(total * train_ratio)
        val_end = train_end + int(total * val_ratio)
        
        splits = {
            'train': indices[:train_end],
            'val': indices[train_end:val_end],
            'test': indices[val_end:]
        }
        
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :")
        print(f"   Train: {len(splits['train'])} images")
        print(f"   Val: {len(splits['val'])} images")
        print(f"   Test: {len(splits['test'])} images")
        
        return splits
    
    def create_yaml_config(self):
        """YOLO í•™ìŠµìš© YAML ì„¤ì • íŒŒì¼ ìƒì„±"""
        config = {
            'path': str(self.base_path.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'nc': len(self.class_names),
            'names': self.class_names
        }
        
        yaml_path = self.base_path / f"{self.dataset_name}.yaml"
        with open(yaml_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        print(f"ğŸ“ YAML ì„¤ì • íŒŒì¼ ìƒì„±: {yaml_path}")
        return yaml_path


class YOLOFineTuner:
    """
    YOLO11 íŒŒì¸íŠœë‹ í´ë˜ìŠ¤
    ê¸°ì¡´ ëª¨ë¸ì„ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¬í•™ìŠµ
    """
    
    def __init__(self, base_model="yolo11n.pt", device="auto"):
        """
        ì´ˆê¸°í™”
        
        Args:
            base_model: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤ (auto, cpu, cuda)
        """
        self.base_model = base_model
        
        if device == "auto":
            self.device = 0 if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.model = None
        self.training_history = []
        
        print(f"ğŸš€ íŒŒì¸íŠœë‹ ì¤€ë¹„")
        print(f"   Base model: {base_model}")
        print(f"   Device: {self.device}")
        
    def configure_training(self, epochs=100, batch_size=16, imgsz=640,
                         patience=50, learning_rate=0.01):
        """
        í•™ìŠµ ì„¤ì • êµ¬ì„±
        
        Args:
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            imgsz: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
            patience: Early stopping patience
            learning_rate: í•™ìŠµë¥ 
        """
        self.training_config = {
            'epochs': epochs,
            'batch': batch_size,
            'imgsz': imgsz,
            'patience': patience,
            'lr0': learning_rate,
            'lrf': 0.01,  # ìµœì¢… í•™ìŠµë¥ 
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'box': 0.05,
            'cls': 0.5,
            'cls_pw': 1.0,
            'obj': 1.0,
            'obj_pw': 1.0,
            'iou_t': 0.20,
            'anchor_t': 4.0,
            'fl_gamma': 0.0,
            'hsv_h': 0.015,
            'hsv_s': 0.7,
            'hsv_v': 0.4,
            'degrees': 0.0,
            'translate': 0.1,
            'scale': 0.5,
            'shear': 0.0,
            'perspective': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,
            'mosaic': 1.0,
            'mixup': 0.0,
            'copy_paste': 0.0
        }
        
        print(f"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:")
        print(f"   Epochs: {epochs}")
        print(f"   Batch size: {batch_size}")
        print(f"   Image size: {imgsz}")
        print(f"   Learning rate: {learning_rate}")
        
    def train(self, data_yaml, project_name="custom_training",
             pretrained=True, resume=False):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            data_yaml: ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„
            pretrained: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
            resume: ì´ì „ í•™ìŠµ ì¬ê°œ ì—¬ë¶€
        """
        print(f"\nğŸ¯ í•™ìŠµ ì‹œì‘: {project_name}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(self.base_model)
        
        # í•™ìŠµ ì‹¤í–‰
        results = self.model.train(
            data=data_yaml,
            project=f"runs/{project_name}",
            name=f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            exist_ok=True,
            pretrained=pretrained,
            device=self.device,
            **self.training_config
        )
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        self.training_history.append({
            'timestamp': datetime.now().isoformat(),
            'model': self.base_model,
            'epochs': self.training_config['epochs'],
            'results': results
        })
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
        
        return results
    
    def validate(self, data_yaml):
        """
        ëª¨ë¸ ê²€ì¦
        
        Args:
            data_yaml: ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼
        """
        if self.model is None:
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ” ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
        
        metrics = self.model.val(
            data=data_yaml,
            device=self.device,
            batch=self.training_config['batch'],
            imgsz=self.training_config['imgsz']
        )
        
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
        print(f"   mAP@0.5: {metrics.box.map50:.4f}")
        print(f"   mAP@0.5-0.95: {metrics.box.map:.4f}")
        print(f"   Precision: {metrics.box.mp:.4f}")
        print(f"   Recall: {metrics.box.mr:.4f}")
        
        return metrics
    
    def export_model(self, format='onnx', output_path=None):
        """
        ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
        
        Args:
            format: ë‚´ë³´ë‚¼ í˜•ì‹ (onnx, torchscript, tflite, etc.)
            output_path: ì €ì¥ ê²½ë¡œ
        """
        if self.model is None:
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ’¾ ëª¨ë¸ ë‚´ë³´ë‚´ê¸°: {format}")
        
        exported_model = self.model.export(
            format=format,
            device=self.device,
            imgsz=self.training_config['imgsz']
        )
        
        if output_path:
            shutil.move(exported_model, output_path)
            print(f"âœ… ëª¨ë¸ ì €ì¥: {output_path}")
        
        return exported_model


class CustomObjectDetector:
    """
    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì»¤ìŠ¤í…€ ê°ì²´ ê²€ì¶œê¸°
    ë” ì •í™•í•œ ê²€ì¶œì„ ìœ„í•œ ê³ ê¸‰ í›„ì²˜ë¦¬ í¬í•¨
    """
    
    def __init__(self, model_path, class_names, confidence_threshold=0.5):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            confidence_threshold: ê¸°ë³¸ ì‹ ë¢°ë„ ì„ê³„ê°’
        """
        self.model = YOLO(model_path)
        self.class_names = class_names
        self.conf_threshold = confidence_threshold
        
        # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ì„ê³„ê°’ (ë” ì •í™•í•œ ê²€ì¶œì„ ìœ„í•´)
        self.class_thresholds = {name: confidence_threshold 
                                for name in class_names}
        
        # í›„ì²˜ë¦¬ ì„¤ì •
        self.nms_threshold = 0.45
        self.max_detections = 100
        
        print(f"ğŸ¯ ì»¤ìŠ¤í…€ ê²€ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ëª¨ë¸: {model_path}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {len(class_names)}")
        
    def set_class_threshold(self, class_name, threshold):
        """íŠ¹ì • í´ë˜ìŠ¤ì˜ ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •"""
        if class_name in self.class_names:
            self.class_thresholds[class_name] = threshold
            print(f"âœ… {class_name} í´ë˜ìŠ¤ ì„ê³„ê°’ ì„¤ì •: {threshold}")
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” í´ë˜ìŠ¤: {class_name}")
    
    def detect(self, image_path, apply_enhancement=True):
        """
        í–¥ìƒëœ ê°ì²´ ê²€ì¶œ
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            apply_enhancement: ì´ë¯¸ì§€ í–¥ìƒ ì ìš© ì—¬ë¶€
        
        Returns:
            ê²€ì¶œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if apply_enhancement:
            image = self._enhance_image(image_path)
        else:
            image = cv2.imread(image_path)
        
        # ê²€ì¶œ ìˆ˜í–‰
        results = self.model(
            image,
            conf=min(self.class_thresholds.values()),
            iou=self.nms_threshold,
            max_det=self.max_detections,
            verbose=False
        )
        
        # í›„ì²˜ë¦¬
        processed_results = self._postprocess(results[0])
        
        # ì‹œê°í™”
        self._visualize_detections(image, processed_results)
        
        return processed_results
    
    def _enhance_image(self, image_path):
        """
        ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
        
        Returns:
            í–¥ìƒëœ ì´ë¯¸ì§€
        """
        image = cv2.imread(image_path)
        
        # CLAHE (Contrast Limited Adaptive Histogram Equalization)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        l = clahe.apply(l)
        enhanced = cv2.merge([l, a, b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        return enhanced
    
    def _postprocess(self, results):
        """
        ê²€ì¶œ ê²°ê³¼ í›„ì²˜ë¦¬
        
        Args:
            results: YOLO ê²€ì¶œ ê²°ê³¼
        
        Returns:
            ì²˜ë¦¬ëœ ê²€ì¶œ ê²°ê³¼
        """
        processed = []
        
        if results.boxes is not None:
            for box in results.boxes:
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                cls_id = int(box.cls[0])
                cls_name = self.class_names[cls_id] if cls_id < len(self.class_names) else 'unknown'
                confidence = float(box.conf[0])
                
                # í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ í™•ì¸
                if confidence >= self.class_thresholds.get(cls_name, self.conf_threshold):
                    # ì¶”ê°€ ê²€ì¦ (ì˜ˆ: ìµœì†Œ í¬ê¸° í™•ì¸)
                    width = x2 - x1
                    height = y2 - y1
                    
                    if width > 10 and height > 10:  # ìµœì†Œ í¬ê¸° í•„í„°
                        processed.append({
                            'class': cls_name,
                            'confidence': confidence,
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'center': [(x1 + x2) / 2, (y1 + y2) / 2],
                            'area': width * height,
                            'aspect_ratio': width / height if height > 0 else 0
                        })
        
        # ì¤‘ë³µ ì œê±° (ì¶”ê°€ NMS)
        processed = self._remove_duplicates(processed)
        
        return processed
    
    def _remove_duplicates(self, detections, iou_threshold=0.5):
        """
        ì¤‘ë³µ ê²€ì¶œ ì œê±°
        
        Args:
            detections: ê²€ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            iou_threshold: IoU ì„ê³„ê°’
        
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ ê²€ì¶œ ê²°ê³¼
        """
        if len(detections) <= 1:
            return detections
        
        # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        kept = []
        for det in detections:
            is_duplicate = False
            
            for kept_det in kept:
                if det['class'] == kept_det['class']:
                    iou = self._calculate_iou(det['bbox'], kept_det['bbox'])
                    if iou > iou_threshold:
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                kept.append(det)
        
        return kept
    
    def _calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def _visualize_detections(self, image, detections):
        """ê²€ì¶œ ê²°ê³¼ ì‹œê°í™”"""
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        ax.imshow(image_rgb)
        ax.axis('off')
        
        # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
        colors = plt.cm.hsv(np.linspace(0, 1, len(self.class_names)))
        color_map = {name: colors[i] for i, name in enumerate(self.class_names)}
        
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            color = color_map.get(det['class'], 'red')
            
            # ë°”ìš´ë”© ë°•ìŠ¤
            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,
                                linewidth=2, edgecolor=color,
                                facecolor='none', alpha=0.8)
            ax.add_patch(rect)
            
            # ë¼ë²¨
            label = f"{det['class']}: {det['confidence']:.3f}"
            ax.text(x1, y1-5, label, fontsize=10,
                   color='white', backgroundcolor=color[:3],
                   bbox=dict(boxstyle="round,pad=0.3",
                           facecolor=color[:3], alpha=0.7))
        
        plt.title(f"Custom Detection: {len(detections)} objects")
        plt.tight_layout()
        plt.show()


class AutoFineTuningPipeline:
    """
    ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
    ë°ì´í„° ì¤€ë¹„ë¶€í„° í•™ìŠµ, í‰ê°€ê¹Œì§€ ìë™í™”
    """
    
    def __init__(self, project_name="auto_finetuning"):
        """
        ì´ˆê¸°í™”
        
        Args:
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„
        """
        self.project_name = project_name
        self.dataset_preparer = None
        self.fine_tuner = None
        self.detector = None
        self.results = {}
        
        print(f"ğŸ¤– ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”: {project_name}")
    
    def prepare_dataset(self, images_dir, annotations_file, 
                       class_names, format_type="coco"):
        """
        ë°ì´í„°ì…‹ ì¤€ë¹„
        
        Args:
            images_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            annotations_file: ì–´ë…¸í…Œì´ì…˜ íŒŒì¼
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            format_type: ì–´ë…¸í…Œì´ì…˜ í˜•ì‹
        """
        print("\nğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        self.dataset_preparer = CustomDatasetPreparer(self.project_name)
        self.dataset_preparer.add_custom_classes(class_names)
        self.dataset_preparer.prepare_annotation_data(
            images_dir, annotations_file, format_type
        )
        
        # ë°ì´í„°ì…‹ ë¶„í• 
        splits = self.dataset_preparer.split_dataset()
        
        # YAML ì„¤ì • ìƒì„±
        yaml_path = self.dataset_preparer.create_yaml_config()
        
        self.results['dataset'] = {
            'yaml_path': str(yaml_path),
            'splits': splits,
            'classes': class_names
        }
        
        return yaml_path
    
    def run_training(self, base_model="yolo11n.pt", epochs=100, 
                    batch_size=16, learning_rate=0.01):
        """
        í•™ìŠµ ì‹¤í–‰
        
        Args:
            base_model: ê¸°ë³¸ ëª¨ë¸
            epochs: í•™ìŠµ ì—í­
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
        """
        print("\nğŸ“ í•™ìŠµ ì‹œì‘...")
        
        self.fine_tuner = YOLOFineTuner(base_model)
        self.fine_tuner.configure_training(
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate
        )
        
        # í•™ìŠµ ì‹¤í–‰
        training_results = self.fine_tuner.train(
            data_yaml=self.results['dataset']['yaml_path'],
            project_name=self.project_name
        )
        
        # ê²€ì¦
        val_metrics = self.fine_tuner.validate(
            self.results['dataset']['yaml_path']
        )
        
        self.results['training'] = {
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'metrics': {
                'mAP50': float(val_metrics.box.map50),
                'mAP50-95': float(val_metrics.box.map),
                'precision': float(val_metrics.box.mp),
                'recall': float(val_metrics.box.mr)
            }
        }
        
        return training_results
    
    def evaluate_model(self, test_images_dir):
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            test_images_dir: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        """
        print("\nğŸ“ˆ ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        if self.fine_tuner.model is None:
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì»¤ìŠ¤í…€ ê²€ì¶œê¸° ìƒì„±
        self.detector = CustomObjectDetector(
            model_path=self.fine_tuner.model,
            class_names=self.results['dataset']['classes']
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í‰ê°€
        test_results = []
        test_images = list(Path(test_images_dir).glob("*.jpg")) + \
                     list(Path(test_images_dir).glob("*.png"))
        
        for img_path in test_images[:5]:  # ìƒ˜í”Œë¡œ 5ê°œë§Œ
            detections = self.detector.detect(str(img_path))
            test_results.append({
                'image': str(img_path),
                'detections': len(detections),
                'classes_found': list(set(d['class'] for d in detections))
            })
        
        self.results['evaluation'] = test_results
        
        return test_results
    
    def generate_report(self):
        """
        íŒŒì¸íŠœë‹ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        """
        print("\nğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = f"""
        ====================================
        íŒŒì¸íŠœë‹ ê²°ê³¼ ë¦¬í¬íŠ¸
        ====================================
        
        í”„ë¡œì íŠ¸: {self.project_name}
        ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        
        1. ë°ì´í„°ì…‹ ì •ë³´
        ----------------
        í´ë˜ìŠ¤ ìˆ˜: {len(self.results['dataset']['classes'])}
        í´ë˜ìŠ¤: {', '.join(self.results['dataset']['classes'])}
        
        2. í•™ìŠµ ê²°ê³¼
        ------------
        Epochs: {self.results['training']['epochs']}
        Batch Size: {self.results['training']['batch_size']}
        Learning Rate: {self.results['training']['learning_rate']}
        
        ì„±ëŠ¥ ì§€í‘œ:
        - mAP@0.5: {self.results['training']['metrics']['mAP50']:.4f}
        - mAP@0.5-0.95: {self.results['training']['metrics']['mAP50-95']:.4f}
        - Precision: {self.results['training']['metrics']['precision']:.4f}
        - Recall: {self.results['training']['metrics']['recall']:.4f}
        
        3. í‰ê°€ ê²°ê³¼
        ------------
        í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜: {len(self.results.get('evaluation', []))}
        """
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = f"reports/{self.project_name}_report.txt"
        os.makedirs("reports", exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        print(report)
        
        return report


def main():
    """
    íŒŒì¸íŠœë‹ ì˜ˆì œ ì‹¤í–‰
    """
    print("ğŸš€ YOLO11 íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ")
    print("="*50)
    
    # ìë™ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = AutoFineTuningPipeline("my_custom_detector")
    
    # ì˜ˆì œ: ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ì •ì˜
    custom_classes = [
        "custom_object_1",
        "custom_object_2",
        "custom_object_3"
    ]
    
    print("\nì»¤ìŠ¤í…€ í´ë˜ìŠ¤:")
    for i, cls in enumerate(custom_classes):
        print(f"  {i}: {cls}")
    
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("1. images/ í´ë”ì— í•™ìŠµ ì´ë¯¸ì§€ ì¤€ë¹„")
    print("2. annotations.json íŒŒì¼ì— COCO í˜•ì‹ ì–´ë…¸í…Œì´ì…˜ ì¤€ë¹„")
    print("3. pipeline.prepare_dataset() ì‹¤í–‰")
    print("4. pipeline.run_training() ì‹¤í–‰")
    print("5. pipeline.evaluate_model() ì‹¤í–‰")
    print("6. pipeline.generate_report() ì‹¤í–‰")
    
    print("\nğŸ“Œ íŒŒì¸íŠœë‹ìœ¼ë¡œ ë‹¤ìŒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:")
    print("- íŠ¹ì • ê°ì²´ì— ëŒ€í•œ ê²€ì¶œ ì •í™•ë„ í–¥ìƒ")
    print("- ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¶”ê°€")
    print("- ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ ìƒì„±")
    print("- ê²€ì¶œ ì†ë„ ìµœì í™”")


if __name__ == "__main__":
    main()